{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reddit_extract_keywords.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"18orQAM8Hu36d58z8IMph-qvKuL__mTaW","authorship_tag":"ABX9TyOQMTiadI93XGxsNiXrLNFU"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"Oe_DJ9dTuBp0"},"source":["!pip3 install pandarallel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igQHDyeMyaVs"},"source":["from pandarallel import pandarallel\r\n","import re\r\n","import string\r\n","import sys\r\n","import html\r\n","import pandas as pd\r\n","import json\r\n","import csv\r\n","import glob\r\n","import os.path\r\n","import time\r\n","import nltk\r\n","import logging as lg\r\n","from datetime import datetime as dt, timedelta\r\n","from gensim.summarization import keywords\r\n","from random import *\r\n","import numpy as np\r\n","from gensim.utils import simple_preprocess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GruVr-m-yVCY","executionInfo":{"status":"ok","timestamp":1612983996028,"user_tz":-330,"elapsed":1407,"user":{"displayName":"Anjay Goel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1pFySUGH_y3sCQpJAfcoMVwRokGP3PSsMUZwusA=s64","userId":"10597040214097389700"}},"outputId":"1be9e4d4-2bd3-4677-c8df-f7cca65ec434"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","#drive.flush_and_unmount()\r\n","pd.set_option('display.max_rows', 500)\r\n","pd.set_option('display.max_columns', 500)\r\n","pd.set_option('display.width', 1000)\r\n","pandarallel.initialize()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hp3aMNvRyyR8"},"source":["df = pd.read_csv(\"/content/drive/My Drive/datasets/dataset_processed.csv\",engine=\"python\")\r\n","df[\"subreddit\"] = df[\"subreddit\"].str.lower()\r\n","df[\"text\"] = df[\"text\"].apply(lambda x: '\\n'.join(preprocess(x.splitlines())))\r\n","print(df.info)\r\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94n1JR6kyDl7"},"source":["subs_arr = [\"unpopularopinion\",\"explainlikeimfive\",\"oldschoolcool\",\"aww\",\"funny\",\"politics\",\"politicalhumor\",\"politicalcompassMemes\",\"conservative\",\r\n","            \"therightcantmeme\",\"neoliberal\",\"democrats\",\"politicaldiscussion\",\r\n","            \"republicans\", \"libertarian\",\"askreddit\",\"showerthoughts\",\r\n","           \"askscience\",\"economics\",\"askeconomics\",\"badeconomics\",\"programmerhumor\",\r\n","            \"interestingasfuck\",\"askwomen\"]\r\n","            \r\n","pol_arr = [\"politics\",\"politicalhumor\",\"politicalcompassMemes\",\"conservative\",\r\n","            \"therightcantmeme\",\"neoliberal\",\"democrats\",\"politicaldiscussion\",\r\n","            \"republicans\", \"libertarian\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QN9J_kzvzAnv"},"source":["punc = '''!()[]|{};\":\\<>/@#$%^&*_~'''\r\n","\r\n","relp = \" \"*len(punc)\r\n","def preprocess(s):\r\n","  result = re.sub(r\"â€™\", \"'\", str(s))\r\n","  result = re.sub(r\"\\n\", ' ', str(result))\r\n","  result = re.sub(r\"r/\", ' ', str(result))\r\n","  result = result.encode(\"ascii\", \"ignore\").decode()\r\n","  result = re.sub(r\"http\\S+\", ' ', str(result), flags=re.MULTILINE)\r\n","  result = html.unescape(result)\r\n","  result = result.translate(str.maketrans(punc,relp))\r\n","  result = re.sub(' +', ' ',str(result))\r\n","  result = result.strip()\r\n","  return result;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGM1NkmWKcwm"},"source":["def keywords_tr(x):\r\n","  kws = keywords(x,ratio=1,lemmatize=True,scores=True)[:10]\r\n","  return json.dumps(dict(kws))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuRAs8NFPv3l"},"source":["TextRank is ridiculously slow. And I can't find any gpu implementations of it. So processing in chunks using multiprocessing."]},{"cell_type":"code","metadata":{"id":"o89iHO4M8eZZ"},"source":["def split_dataframe(df, chunk_size = 5000): \r\n","    chunks = list()\r\n","    num_chunks = len(df) // chunk_size + 1\r\n","    for i in range(num_chunks):\r\n","        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\r\n","    return chunks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wds4OPDVre6z"},"source":["i = 0\r\n","for df_chunk in split_dataframe(df,chunk_size=1000):\r\n","  print(i)\r\n","  df_chunk[\"text\"]=df_chunk[\"text\"].parallel_apply(lambda x: keywords_tr(x))\r\n","  df_chunk.to_csv(f\"/content/drive/My Drive/datasets/keywords/{i}.csv\",index=False)\r\n","  i+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONpXOpNARbrF"},"source":["path = r'/content/drive/My Drive/datasets/keywords/' \r\n","all_files = glob.glob(path + \"/*.csv\")\r\n","\r\n","li = []\r\n","\r\n","for filename in all_files:\r\n","    df = pd.read_csv(filename, index_col=None, header=0)\r\n","    li.append(df)\r\n","dff = pd.concat(li, axis=0, ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYN3u9xrYngv"},"source":["dff[\"keywords\"] = dff[\"text\"].apply(lambda x: list(json.loads(x).keys()))\r\n","dff[\"scores\"] = dff[\"text\"].apply(lambda x: list(json.loads(x).values()))\r\n","dff[\"label\"]=dff[\"subreddit\"].apply(lambda x:x in pol_arr)\r\n","dff.drop(columns=[\"text\"],inplace=True)\r\n","dff.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUGYk3Nucgex"},"source":["dff.to_csv(\"/content/drive/My Drive/datasets/keywords.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEX0n-O3d7Ax"},"source":["def flatten_lst(lst):\r\n","  ret_list = []\r\n","  for i in lst:\r\n","    ret_list.extend(i)\r\n","  return ret_list\r\n","\r\n","def split_words(lst):\r\n","  ret_list = []\r\n","  for i in lst:\r\n","    ret_list.extend(i.split())\r\n","  return ret_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeFZcgEGeHIq"},"source":["Extract Individual Words"]},{"cell_type":"code","metadata":{"id":"zekzOC6leF4u"},"source":["df[\"keywords\"]=df[\"keywords\"].apply(split_words)\r\n","pol_lst = flatten_lst(list(df[df[\"label\"]==1][\"keywords\"]))\r\n","df_pol = pd.DataFrame(dict(Counter(pol_lst)).items(), columns=['word','pol_freq'])\r\n","df_pol.set_index('word',inplace=True)\r\n","\r\n","non_pol_lst = flatten_lst(list(df[df[\"label\"]==0][\"keywords\"]))\r\n","df_non_pol = pd.DataFrame(dict(Counter(non_pol_lst)).items(), columns=['word','non_pol_freq'])\r\n","df_non_pol.set_index('word',inplace=True)\r\n","\r\n","result = pd.concat([df_non_pol, df_pol], axis=1)\r\n","result.reset_index(inplace=True)\r\n","result.fillna(0,inplace=True)\r\n","result[\"total_freq\"]=result[\"pol_freq\"]+result[\"non_pol_freq\"]\r\n","result.sort_values(by=['total_freq'],ignore_index=True,inplace=True,ascending=False)\r\n","print(result.head(100))\r\n","result.to_csv(\"/content/drive/MyDrive/datasets/keywords_freq.csv\")"],"execution_count":null,"outputs":[]}]}