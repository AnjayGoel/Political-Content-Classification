{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "reddit_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEw9cLLQbbhs"
      },
      "source": [
        "!unzip /content/drive/MyDrive/datasets/glove.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boMMrdEZP0s6"
      },
      "source": [
        "!apt install libomp-dev\r\n",
        "!pip install faiss\r\n",
        "!pip install faiss-gpu\r\n",
        "import faiss  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeb3Ip__Ydrg"
      },
      "source": [
        "!pip install --upgrade xgboost\r\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ngYYYWPSmy"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import sys\r\n",
        "import json\r\n",
        "import csv\r\n",
        "import gc\r\n",
        "from collections import Counter\r\n",
        "import glob\r\n",
        "import os.path\r\n",
        "import time\r\n",
        "import nltk\r\n",
        "from random import *\r\n",
        "import ast\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "from google.colab import drive\r\n",
        "import requests_oauthlib\r\n",
        "from scipy.spatial import distance\r\n",
        "from tensorflow.keras.initializers import Constant\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.linear_model import LogisticRegression,LinearRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import datasets, layers, models\r\n",
        "import tensorflow as tf\r\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnOxsO3pQG0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdc0eab-8ccf-4f5a-b9a3-566315030b53"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "#drive.flush_and_unmount()\r\n",
        "pd.set_option('display.max_rows', 5000)\r\n",
        "pd.set_option('display.max_columns', 5000)\r\n",
        "pd.set_option('display.width', 1000)\r\n",
        "pd.set_option('display.max_colwidth', -1)  # or 199"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GezGGOE6hs1l"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/datasets/keywords_freq.csv\")\r\n",
        "df[\"rel\"] = (df[\"pol_freq\"]*df[\"total_freq\"].sum())/(df[\"total_freq\"]*df[\"pol_freq\"].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd0T1NgBqrvp"
      },
      "source": [
        "wfd = dict(zip(df.word, df.total_freq))\r\n",
        "def get_most_common(x):  #get most frequent word from a phrase.\r\n",
        "  if len(x)<2:\r\n",
        "    return x\r\n",
        "  freq = 0\r\n",
        "  word_f = x[0]\r\n",
        "  for word in x.split():\r\n",
        "    if (word in [\"null\",\"nan\"]):\r\n",
        "      continue\r\n",
        "    elif (wfd[word]>freq):\r\n",
        "      word_f = word\r\n",
        "      freq = wfd[word]\r\n",
        "  return word_f\r\n",
        "\r\n",
        "def split_words(lst):\r\n",
        "  ret_list = []\r\n",
        "  for i in lst:\r\n",
        "    ret_list.append(get_most_common(i))\r\n",
        "  return ret_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym2KO6PrPncP"
      },
      "source": [
        "df_main = pd.read_csv(\"/content/drive/MyDrive/datasets/keywords.csv\")\r\n",
        "df_main.keywords = df_main.keywords.apply(ast.literal_eval)\r\n",
        "df_main.scores = df_main.scores.apply(ast.literal_eval)\r\n",
        "df_main[\"keywords\"] = df_main[\"keywords\"].apply(split_words)\r\n",
        "df_main = df_main.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbALuN8Xfunf"
      },
      "source": [
        "split_pos = 0.01 #imbalance test set\n",
        "split_neg = 0.5\n",
        "\n",
        "df_main = df_main[~df_main[\"subreddit\"].isin((\"badeconomics\",\"askeconomics\", \"economics\",\"politicalcompassMemes\"))]\n",
        "df_pos = df_main[df_main[\"label\"]==1]\n",
        "df_neg = df_main[df_main[\"label\"]==0]\n",
        "\n",
        "df_pos_train,df_pos_test = train_test_split(df_pos, test_size=split_pos)\n",
        "df_neg_train,df_neg_test = train_test_split(df_neg, test_size=split_neg)\n",
        "\n",
        "df_train = pd.concat([df_pos_train, df_neg_train], ignore_index=True)\n",
        "df_test = pd.concat([df_pos_test, df_neg_test], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYZt7mH-AUI7"
      },
      "source": [
        "print(df_pos_train.info())\r\n",
        "print(df_pos_test.info())\r\n",
        "print(df_neg_train.info())\r\n",
        "print(df_neg_test.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04akOrhUb9Dp"
      },
      "source": [
        "rel = dict(zip(df.word, df.rel))\r\n",
        "\r\n",
        "def get_params(x,score):\r\n",
        "  try:\r\n",
        "    ret = [rel[x]] #[0 if rel[x]<1.5 else 1]\r\n",
        "  except:\r\n",
        "    ret = [0.0]\r\n",
        "  return ret\r\n",
        "def get_x(xs,scores):\r\n",
        "  ret_val = []\r\n",
        "  for i in range(len(xs)):\r\n",
        "    ret_val.extend(get_params(xs[i],scores[i]))\r\n",
        "  ret_val = np.sort(np.array(ret_val))\r\n",
        "  ret_val = ret_val[::-1]\r\n",
        "  ret_val = np.pad(ret_val, (0, max(0,10-len(ret_val))), 'constant') #[np.sum(ret_val)/10]\r\n",
        "  return ret_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbKdHdpuhqLF"
      },
      "source": [
        "x_train = np.stack(df_train.apply(lambda row: get_x(row[\"keywords\"],row[\"scores\"]), axis=1))\r\n",
        "y_train = np.array(df_train.label.astype(int))\r\n",
        "\r\n",
        "x_test = np.stack(df_test.apply(lambda row: get_x(row[\"keywords\"],row[\"scores\"]), axis=1))\r\n",
        "y_test = np.array(df_test.label.astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IjFuukwsF36"
      },
      "source": [
        "def plot_cm(cf_matrix):\r\n",
        "  group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\r\n",
        "  group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\r\n",
        "  group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\r\n",
        "  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\r\n",
        "  labels = np.asarray(labels).reshape(2,2)\r\n",
        "  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_JOnsCV2qV"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siAG1r7bBtO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "5df51cca-1e73-4242-ffd5-255f3dbb92af"
      },
      "source": [
        "lr = LogisticRegression(random_state=0,max_iter=200)\r\n",
        "lr.fit(x_train, y_train)\r\n",
        "print(f\"Prediction Accuracy: {lr.score(x_test, y_test)}\")\r\n",
        "y_pred = lr.predict(x_test)\r\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\r\n",
        "plot_cm(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOxtPnT5HO9b"
      },
      "source": [
        "wrong = df_test[(y_pred==0) & (y_test == 1)].keywords\r\n",
        "for i in wrong.sample(n=28):\r\n",
        "  #pass\r\n",
        "  #print(i)\r\n",
        "  print(dict(zip(i, get_x(i,[i for i in range(10)]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMj9thCi2Bvo"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsno1UUT2EYC"
      },
      "source": [
        "class FaissKNeighbors:\r\n",
        "    def __init__(self, k=5):\r\n",
        "        self.index = None\r\n",
        "        self.y = None\r\n",
        "        self.k = k\r\n",
        "\r\n",
        "    def fit(self, X, y):\r\n",
        "        self.index = faiss.IndexFlatL2(X.shape[1])\r\n",
        "        self.index.add(X.astype(np.float32))\r\n",
        "        self.y = y\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\r\n",
        "        votes = self.y[indices]\r\n",
        "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])\r\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-aURq_6349u"
      },
      "source": [
        "fknn = FaissKNeighbors(k=5)\r\n",
        "fknn.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zmbJoQEYxN"
      },
      "source": [
        "y_pred = fknn.predict(x_test)\r\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\r\n",
        "print(metrics.accuracy_score(y_test,y_pred))\r\n",
        "plot_cm(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEly0OdTnlNg"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5B_IwKsnoWl"
      },
      "source": [
        "xgb = XGBClassifier(n_estimators=100)\r\n",
        "training_start = time.perf_counter()\r\n",
        "xgb.fit(x_train, y_train)\r\n",
        "training_end = time.perf_counter()\r\n",
        "prediction_start = time.perf_counter()\r\n",
        "y_pred = xgb.predict(x_test)\r\n",
        "prediction_end = time.perf_counter()\r\n",
        "acc_xgb = (y_pred == y_test).sum().astype(float) / len(y_pred)*100\r\n",
        "xgb_train_time = training_end-training_start\r\n",
        "xgb_prediction_time = prediction_end-prediction_start\r\n",
        "print(f\"XGBoost's prediction accuracy is: {acc_xgb}\")\r\n",
        "print(f\"Time consumed for training: {xgb_train_time}\")\r\n",
        "print(f\"Time consumed for prediction: {xgb_prediction_time}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ9rYkXxVVdO"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NBWPbR-VX46"
      },
      "source": [
        "clf=RandomForestClassifier(n_estimators=50,max_depth=40)\r\n",
        "clf.fit(x_train,y_train)\r\n",
        "y_pred=clf.predict(x_test)\r\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73uLs-MWUGf"
      },
      "source": [
        "Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5EXX-V_WXUS"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(30))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(10))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajJDqB-kYzde"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=3,batch_size=10, verbose=1)\r\n",
        "_, accuracy = model.evaluate(x_test, y_test)\r\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFzlKQ9LHYSp"
      },
      "source": [
        "Using Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS0bg65oTVmz"
      },
      "source": [
        "embeddings= {}\r\n",
        "with open(\"/content/glove/glove.840B.300d.txt\", 'r') as f:\r\n",
        "    for line in f:\r\n",
        "        values = line.split(' ')\r\n",
        "        word = values[0]\r\n",
        "        vector = np.asarray(values[1:], \"float32\")\r\n",
        "        embeddings[word]=vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZqW0ulWgVtU"
      },
      "source": [
        "def get_embedding(x):\r\n",
        "  try:\r\n",
        "    return embeddings[x]\r\n",
        "  except:\r\n",
        "    return np.zeros(shape=(300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK-vGYdKOFjZ"
      },
      "source": [
        "word_to_ind = {}\r\n",
        "ind_to_word = {}\r\n",
        "embeddings_ds = []\r\n",
        "\r\n",
        "for ind,word in enumerate(list(df[\"word\"])):\r\n",
        "  word_to_ind[word]=ind\r\n",
        "  ind_to_word[ind]=word\r\n",
        "  embeddings_ds.append(get_embedding(word))\r\n",
        "embeddings_ds = np.array(embeddings_ds,dtype=\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxwbjIU6Qx3f"
      },
      "source": [
        "embeddings_ds[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr5hymHI_is2"
      },
      "source": [
        "index = faiss.IndexFlatL2(300)  \r\n",
        "index.add(embeddings_ds)\r\n",
        "print(index.ntotal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVJF9Gid_lDd"
      },
      "source": [
        "get_nearest(x):\r\n",
        "  D, I = index.search(np.array([get_embedding(x), 1) # sanity check\r\n",
        "  return ind_to_word[I[0][0]]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}